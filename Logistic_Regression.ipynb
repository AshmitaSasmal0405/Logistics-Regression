{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "          -------------THEORY ANSWER--------------\n",
        "\n",
        "\n",
        " 1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "   - Logistic Regression is a statistical model used for binary classification tasks, predicting the probability of a binary outcome (e.g., 0 or 1, true or false). It models the relationship between a set of independent variables and a categorical dependent variable.\n",
        "   * Linear Regression, on the other hand, is used for regression tasks, predicting a continuous outcome variable (e.g., house prices, temperature).\n",
        "   * The key difference lies in their output: Linear Regression outputs a continuous value, while Logistic Regression outputs a probability that is then mapped to a class using a sigmoid function.\n",
        "\n",
        "\n",
        " 2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "\n",
        "   - The core of Logistic Regression is the sigmoid (or logistic) function. The probability P(Y=1|X) that the dependent variable Y is 1 given independent variables X is modeled as:\n",
        "     P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n)}}\n",
        "     Where:\n",
        "     * P(Y=1|X) is the probability of the dependent variable being 1.\n",
        "     * e is the base of the natural logarithm.\n",
        "     * \\beta_0 is the intercept.\n",
        "     * \\beta_1, \\dots, \\beta_n are the coefficients for the independent variables X_1, \\dots, X_n.\n",
        "   * Often, this is also expressed in terms of the log-odds (logit function):\n",
        "     \\ln\\left(\\frac{P(Y=1|X)}{1 - P(Y=1|X)}\\right) = \\beta_0 + \\beta_1X_1 + \\dots + \\beta_nX_n\n",
        "\n",
        "\n",
        " 3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "\n",
        "   -  The Sigmoid function maps any real-valued number into a value between 0 and 1. This is crucial for Logistic Regression because we want to model the probability of a binary outcome, which must be within this range. It effectively squashes the linear combination of inputs into a probability.\n",
        "\n",
        "\n",
        " 4. What is the cost function of Logistic Regression?\n",
        "   * The cost function for Logistic Regression is typically the Log Loss (also known as Cross-Entropy Loss). For a single training example (x, y), where y is the true label (0 or 1) and \\hat{y} is the predicted probability, the cost is:\n",
        "     Cost(\\hat{y}, y) = -(y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}))\n",
        "   * For m training examples, the overall cost function J(\\theta) to be minimized is:\n",
        "     J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]\n",
        "\n",
        "\n",
        "\n",
        " 5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "\n",
        "\n",
        "   -  Regularization is a technique used to prevent overfitting in machine learning models, including Logistic Regression. It adds a penalty term to the cost function, discouraging the model from assigning excessively large weights to features.\n",
        "   * Why it's needed: When a model overfits, it performs very well on the training data but poorly on unseen data because it has learned the noise and specificities of the training set rather than the underlying patterns. Regularization helps to generalize the model better to new data by making the coefficients smaller and less sensitive to minor fluctuations in the input data.\n",
        "\n",
        " 6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        "\n",
        "\n",
        "   - These are different types of regularization techniques:\n",
        "     * Lasso Regression (L1 Regularization): Adds the absolute value of the magnitude of the coefficients as a penalty term to the cost function.\n",
        "       * Penalty: \\lambda \\sum_{j=1}^{n} |\\beta_j|\n",
        "       * Effect: Can shrink some coefficients exactly to zero, effectively performing feature selection by excluding less important features from the model.\n",
        "     * Ridge Regression (L2 Regularization): Adds the squared magnitude of the coefficients as a penalty term to the cost function.\n",
        "       * Penalty: \\lambda \\sum_{j=1}^{n} \\beta_j^2\n",
        "       * Effect: Shrinks coefficients towards zero but does not set them exactly to zero. It reduces the impact of less important features but keeps them in the model.\n",
        "     * Elastic Net Regression: A hybrid of Lasso and Ridge regression, it adds both L1 and L2 penalty terms to the cost function.\n",
        "       * Penalty: \\lambda_1 \\sum_{j=1}^{n} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{n} \\beta_j^2\n",
        "       * Effect: Combines the feature selection ability of Lasso with the coefficient shrinkage and group effect of Ridge. It is particularly useful when there are highly correlated features.\n",
        "\n",
        "\n",
        "  7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        "\n",
        "   -  Elastic Net is preferred in situations where:\n",
        "     * There are many features, and you suspect that only a few are truly relevant, or that there are groups of correlated features. Lasso might arbitrarily select one from a group of correlated features, while Ridge would shrink them all proportionally. Elastic Net can select entire groups of correlated features together.\n",
        "     * You want the feature selection property of Lasso while also benefiting from the stability of Ridge, especially when dealing with high-dimensional data.\n",
        "\n",
        "\n",
        " 8. What is the impact of the regularization parameter (\\lambda) in Logistic Regression?\n",
        "\n",
        "\n",
        "   * The regularization parameter (\\lambda) controls the strength of the regularization.\n",
        "     * Small \\lambda (or \\lambda close to 0): The penalty term has little effect, and the model behaves more like an unregularized Logistic Regression. This can lead to overfitting if the model is complex.\n",
        "     * Large \\lambda: The penalty term has a strong effect, forcing coefficients to be smaller (Ridge) or exactly zero (Lasso). This can lead to underfitting if \\lambda is too large, as the model might be too simple to capture the underlying patterns.\n",
        "   * The goal is to find an optimal \\lambda that balances bias and variance, leading to good generalization performance.\n",
        "\n",
        "\n",
        " 9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "\n",
        "   -  While Logistic Regression has fewer strict assumptions than Linear Regression, some important considerations include:\n",
        "     * Binary Outcome: The dependent variable must be binary or dichotomous.\n",
        "     * Independence of Observations: Observations should be independent of each other.\n",
        "     * No Multicollinearity: Independent variables should not be highly correlated with each other (or high multicollinearity can make coefficient interpretation difficult).\n",
        "     * Linearity of Log-Odds: The independent variables are linearly related to the log-odds of the outcome (the logit of the dependent variable).\n",
        "     * Large Sample Size: Logistic Regression generally performs better with larger sample sizes.\n",
        "\n",
        "\n",
        " 10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "   - Several other powerful algorithms can be used for classification:\n",
        "     * Support Vector Machines (SVM)\n",
        "     * Decision Trees\n",
        "     * Random Forests\n",
        "     * Gradient Boosting Machines (e.g., XGBoost, LightGBM)\n",
        "     * K-Nearest Neighbors (KNN)\n",
        "     * Naive Bayes\n",
        "     * Neural Networks\n",
        "\n",
        "\n",
        " 11. How are Classification Evaluation Metrics used?\n",
        "\n",
        "\n",
        "   - Classification evaluation metrics are used to assess the performance of a classification model. They quantify how well the model predicts the correct classes. Common metrics include:\n",
        "     * Accuracy: The proportion of correctly classified instances out of the total instances.\n",
        "     * Precision: The proportion of true positive predictions among all positive predictions.\n",
        "     * Recall (Sensitivity): The proportion of true positive predictions among all actual positive instances.\n",
        "     * F1-Score: The harmonic mean of precision and recall, providing a balance between the two.\n",
        "     * ROC Curve and AUC (Area Under the Receiver Operating Characteristic Curve): Measures the ability of a classifier to distinguish between classes. A higher AUC indicates better performance.\n",
        "     * Confusion Matrix: A table that summarizes the number of true positives, true negatives, false positives, and false negatives.\n",
        "\n",
        "\n",
        " 12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        "\n",
        "   - Class imbalance occurs when one class significantly outnumbers the other(s) in the training data.\n",
        "   * Impact on Logistic Regression:\n",
        "     * Bias towards Majority Class: Logistic Regression, like many other algorithms, tends to be biased towards the majority class. It might predict the majority class for most instances, even if it leads to poor performance on the minority class, because simply predicting the majority class can yield high overall accuracy.\n",
        "     * Poor Performance on Minority Class: The model might have a very low recall or precision for the minority class, meaning it struggles to correctly identify instances of the less frequent class.\n",
        "   * Mitigation Strategies:\n",
        "     * Resampling Techniques:\n",
        "       * Oversampling minority class: (e.g., SMOTE, ADASYN) creating synthetic samples for the minority class.\n",
        "       * Undersampling majority class: (e.g., Random undersampling) removing samples from the majority class.\n",
        "     * Cost-sensitive learning: Assigning different misclassification costs to different classes.\n",
        "     * Using appropriate evaluation metrics: Focusing on metrics like precision, recall, F1-score, or AUC rather than just accuracy.\n",
        "\n",
        "\n",
        " 13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "\n",
        "   - Hyperparameter tuning is the process of finding the optimal set of hyperparameters for a machine learning model to achieve the best performance. Hyperparameters are parameters whose values are set before the learning process begins, unlike model parameters (e.g., coefficients) that are learned from the data.\n",
        "   * For Logistic Regression, common hyperparameters to tune include:\n",
        "     * Regularization parameter (\\lambda or C, which is 1/\\lambda): Controls the strength of regularization.\n",
        "     * Type of regularization (L1, L2, Elastic Net): Choosing which regularization penalty to apply.\n",
        "     * Solver: The algorithm used to optimize the model.\n",
        "   * Methods for tuning: Grid search, Random search, Bayesian optimization.\n",
        "\n",
        "\n",
        " 14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "\n",
        "   - Solvers are algorithms used to optimize the cost function of Logistic Regression. Different solvers are suitable for different datasets and regularization types. Common solvers include:\n",
        "     * 'liblinear': Good for small datasets. Supports L1 and L2 regularization.\n",
        "     * 'newton-cg': Good for large datasets. Supports L2 regularization.\n",
        "     * 'lbfgs': Default in many libraries (e.g., scikit-learn). Good for large datasets. Supports L2 regularization.\n",
        "     * 'sag' (Stochastic Average Gradient): Good for very large datasets and when the number of features is large. Supports L2 regularization.\n",
        "     * 'saga': A variant of 'sag' that also supports L1 regularization and Elastic Net.\n",
        "   * Which one to use depends on:\n",
        "     * Dataset size: 'liblinear' for small, 'lbfgs', 'newton-cg', 'sag', 'saga' for large.\n",
        "     * Regularization type: 'liblinear' and 'saga' for L1, 'lbfgs', 'newton-cg', 'sag', 'saga' for L2. 'saga' also supports Elastic Net.\n",
        "     * Computational resources: Some solvers might be faster or more memory-efficient depending on the data.\n",
        "   * Often, 'lbfgs' is a good general-purpose choice, and 'saga' is very versatile if L1 or Elastic Net regularization is needed.\n",
        "\n",
        "\n",
        " 15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        "\n",
        "   - Logistic Regression is inherently a binary classifier. For multiclass classification, it can be extended using two main strategies:\n",
        "     * One-vs-Rest (OvR) / One-vs-All (OvA):\n",
        "       * For a K-class problem, OvR trains K separate binary Logistic Regression models.\n",
        "       * Each model is trained to distinguish one class from all the other classes combined.\n",
        "       * During prediction, each of the K models outputs a probability, and the class with the highest probability is chosen as the final prediction.\n",
        "     * Multinomial Logistic Regression (Softmax Regression):\n",
        "       * This is a direct extension of Logistic Regression for multiclass problems, where a single model is trained to predict the probabilities of each class.\n",
        "       * It uses the Softmax function (instead of the Sigmoid) to output a probability distribution over the K classes, where the probabilities sum to 1.\n",
        "       * It models the relationship between the independent variables and the log-odds of each class relative to a reference class.\n",
        "\n",
        "\n",
        " 16. What are the advantages and disadvantages of Logistic Regression?\n",
        "\n",
        "\n",
        "   * Advantages:\n",
        "     * Simplicity and Interpretability: It's relatively easy to understand and implement. The coefficients can be interpreted in terms of log-odds.\n",
        "     * Efficiency: Computationally efficient, especially for large datasets.\n",
        "     * Good Baseline: Often serves as a good baseline model for classification problems.\n",
        "     * Probabilistic Output: Provides probabilities, which can be useful for decision-making.\n",
        "     * Less Prone to Overfitting (with regularization): Can be regularized to prevent overfitting.\n",
        "   * Disadvantages:\n",
        "     * Linearity Assumption: Assumes a linear relationship between independent variables and the log-odds, which might not hold in complex real-world scenarios.\n",
        "     * Sensitive to Outliers: Can be sensitive to outliers, especially if regularization is not used.\n",
        "     * Not suitable for non-linear decision boundaries: Struggles when the data has complex, non-linear relationships.\n",
        "     * Requires feature engineering: May require significant feature engineering to capture non-linear relationships.\n",
        "     * Multicollinearity issues: Can be affected by high multicollinearity among independent variables.\n",
        "\n",
        "\n",
        " 17. What are some use cases of Logistic Regression?\n",
        "\n",
        "\n",
        "   * Logistic Regression is widely used in various fields for binary classification tasks:\n",
        "     * Medical Diagnosis: Predicting the likelihood of a disease (e.g., cancer, diabetes) based on symptoms and test results.\n",
        "     * Customer Churn Prediction: Identifying customers who are likely to churn (cancel a service).\n",
        "     * Credit Scoring: Assessing the probability of a loan applicant defaulting on a loan.\n",
        "     * Spam Detection: Classifying emails as spam or not spam.\n",
        "     * Marketing: Predicting whether a customer will purchase a product or respond to a marketing campaign.\n",
        "     * Fraud Detection: Identifying fraudulent transactions.\n",
        "\n",
        "\n",
        " 18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "\n",
        "   - Logistic Regression:\n",
        "     * Used for binary classification problems (two classes).\n",
        "     * Uses the Sigmoid function to output a single probability between 0 and 1, which is then thresholded to assign a class.\n",
        "     * Models the probability of one class against the other.\n",
        "   * Softmax Regression (Multinomial Logistic Regression):\n",
        "     * Used for multiclass classification problems (more than two classes).\n",
        "     * Uses the Softmax function to output a probability distribution over all classes, where the sum of probabilities for all classes equals 1.\n",
        "     * Directly models the probability of an instance belonging to each of the multiple classes.\n",
        "\n",
        "\n",
        " 19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "\n",
        "   - Theoretical Perspective:\n",
        "     * Softmax Regression is generally preferred when the classes are mutually exclusive, meaning an instance can only belong to one class (e.g., an image is either a cat, a dog, or a bird, but not two simultaneously). It provides a more unified and consistent probabilistic model for multiclass outcomes.\n",
        "     * One-vs-Rest (OvR) is often used when classes are not mutually exclusive (e.g., an email can be both \"work\" and \"urgent\"). However, it's more commonly used for mutually exclusive classes due to its simplicity in implementation. OvR treats each class independently.\n",
        "   -  Practical Considerations:\n",
        "     * Softmax Regression is often more computationally efficient for a large number of classes because it trains only one model.\n",
        "     * OvR trains K separate models, which can be computationally more expensive for a very large K.\n",
        "     * In many machine learning libraries (like scikit-learn), when you use Logistic Regression for multiclass problems, it often defaults to OvR unless explicitly specified to use multinomial (Softmax).\n",
        "     * For mutually exclusive classes, Softmax regression generally yields better-calibrated probabilities.\n",
        "\n",
        "\n",
        " 20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "\n",
        "   - Interpreting coefficients in Logistic Regression is slightly different from Linear Regression because the relationship is with the log-odds of the outcome, not directly with the outcome itself.\n",
        "   * For a coefficient \\beta_j associated with an independent variable X_j:\n",
        "     * Exponentiated Coefficient (Odds Ratio): The most common way to interpret it is by calculating e^{\\beta_j}. This value represents the odds ratio.\n",
        "       * If X_j is a continuous variable, then for a one-unit increase in X_j, the odds of the outcome (Y=1) are multiplied by e^{\\beta_j}, assuming all other variables are held constant.\n",
        "       * If X_j is a binary variable (e.g., 0 or 1), then e^{\\beta_j} is the ratio of the odds of the outcome for X_j=1 compared to X_j=0, assuming all other variables are held constant.\n",
        "     * Positive Coefficient: A positive coefficient means that as the independent variable increases, the log-odds of the outcome (and thus the probability of the outcome) increase.\n",
        "     * Negative Coefficient: A negative coefficient means that as the independent variable increases, the log-odds of the outcome (and thus the probability of the outcome) decrease.\n",
        "   * Example: If the coefficient for 'hours studied' is 0.5, then e^{0.5} \\approx 1.65. This means that for every additional hour studied, the odds of passing the exam increase by 65%, assuming all other factors are constant."
      ],
      "metadata": {
        "id": "gTUvHk7HprsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "             --------------PRACTICAL ANSWER--------------------"
      ],
      "metadata": {
        "id": "90NOCL_T1zfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#1.#Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSZj8OFUtwBP",
        "outputId": "1256a31d-de83-4d5f-e636-9ca67fa72573"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
        "\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# To use Logistic Regression with L1 penalty, restrict to binary classification\n",
        "# Let's select only class 0 and 1 for simplicity\n",
        "binary_filter = y < 2\n",
        "X = X[binary_filter]\n",
        "y = y[binary_filter]\n",
        "\n",
        "# Step 2: Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Apply Logistic Regression with L1 regularization\n",
        "model = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print model accuracy\n",
        "print(f\"L1-Regularized Logistic Regression Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUtgmncTuA7N",
        "outputId": "40d62094-7260-412c-eb4f-baa39c1c5d5e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L1-Regularized Logistic Regression Model Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression with L2 regularization (Ridge)\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=200, multi_class='auto')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"L2-Regularized Logistic Regression Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "# Step 6: Print model coefficients\n",
        "print(\"\\nModel Coefficients (weights):\")\n",
        "print(model.coef_)\n",
        "\n",
        "# Optional: Print intercepts\n",
        "print(\"\\nModel Intercepts:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HpgtIXuuFav",
        "outputId": "7c5ce624-aa68-493c-9c64-b0fb76729fc5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2-Regularized Logistic Regression Accuracy: 1.00\n",
            "\n",
            "Model Coefficients (weights):\n",
            "[[-0.39345607  0.96251768 -2.37512436 -0.99874594]\n",
            " [ 0.50843279 -0.25482714 -0.21301129 -0.77574766]\n",
            " [-0.11497673 -0.70769055  2.58813565  1.7744936 ]]\n",
            "\n",
            "Model Intercepts:\n",
            "[  9.00884295   1.86902164 -10.87786459]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4.Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Use only two classes (binary classification)\n",
        "binary_mask = y < 2  # Use only class 0 and 1\n",
        "X = X[binary_mask]\n",
        "y = y[binary_mask]\n",
        "\n",
        "# Step 3: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train Logistic Regression with Elastic Net regularization\n",
        "model = LogisticRegression(\n",
        "    penalty='elasticnet',\n",
        "    solver='saga',\n",
        "    l1_ratio=0.5,       # Mix of L1 and L2 (0.5 = 50% each)\n",
        "    max_iter=500\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print results\n",
        "print(f\"Elastic Net Logistic Regression Accuracy: {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n",
        "print(\"\\nIntercept:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MzsUxHSyuK9i",
        "outputId": "0b4c034d-15e5-48f1-e51d-c42cccf8daf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elastic Net Logistic Regression Accuracy: 1.00\n",
            "\n",
            "Model Coefficients:\n",
            "[[ 0.         -1.31649663  2.40423452  0.56663786]]\n",
            "\n",
            "Intercept:\n",
            "[-2.86066059]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5.Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (3-class classification)\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Train Logistic Regression with One-vs-Rest strategy\n",
        "model = LogisticRegression(\n",
        "    multi_class='ovr',   # One-vs-Rest multiclass strategy\n",
        "    solver='lbfgs',      # Solver that supports 'ovr'\n",
        "    max_iter=200\n",
        ")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 5: Print results\n",
        "print(f\"Multiclass Logistic Regression Accuracy (OvR): {accuracy:.2f}\")\n",
        "print(\"\\nModel Coefficients:\")\n",
        "print(model.coef_)\n",
        "print(\"\\nModel Intercepts:\")\n",
        "print(model.intercept_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzanSdZ3uQK3",
        "outputId": "0825ac98-d807-4f56-bcd1-d8f5908f2ecc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multiclass Logistic Regression Accuracy (OvR): 0.97\n",
            "\n",
            "Model Coefficients:\n",
            "[[-0.42762216  0.88771927 -2.21471658 -0.91610036]\n",
            " [-0.03387836 -2.0442989   0.54266011 -1.0179372 ]\n",
            " [-0.38904645 -0.62147609  2.7762982   2.09067085]]\n",
            "\n",
            "Model Intercepts:\n",
            "[  6.24415327   4.81099217 -12.83530153]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accura\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define model\n",
        "logreg = LogisticRegression(solver='saga', max_iter=500)\n",
        "\n",
        "# Step 4: Define parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],                # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Types of penalties\n",
        "    'l1_ratio': [None, 0.5]                 # Required for 'elasticnet', ignored otherwise\n",
        "}\n",
        "\n",
        "# Step 5: Grid Search with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Best parameters and accuracy\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYsQR_OxyJNT",
        "outputId": "decaa977-9150-4940-e2aa-5cf0644db7a8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l1)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 1, 'l1_ratio': None, 'penalty': 'l1'}\n",
            "Test Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "20 fits failed out of a total of 120.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "20 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.33333333 0.85833333        nan 0.325      0.85833333 0.63333333\n",
            " 0.925      0.94166667        nan 0.925      0.94166667 0.94166667\n",
            " 0.975      0.975             nan 0.975      0.975      0.975\n",
            " 0.96666667 0.96666667        nan 0.96666667 0.96666667 0.96666667]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7.Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Define model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Step 3: Define Stratified K-Fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 4: Perform cross-validation\n",
        "scores = cross_val_score(model, X, y, cv=skf, scoring='accuracy')\n",
        "\n",
        "# Step 5: Print accuracy results\n",
        "print(\"Cross-Validation Accuracies:\", scores)\n",
        "print(f\"Average Accuracy: {np.mean(scores):.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIQfTeKPuccP",
        "outputId": "e4ae62e1-c195-403a-b98a-828d4fac97a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross-Validation Accuracies: [1.         0.96666667 0.93333333 1.         0.93333333]\n",
            "Average Accuracy: 0.97\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8.Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset from CSV\n",
        "# Replace 'your_file.csv' with the path to your CSV file\n",
        "data = pd.read_csv('your_file.csv')\n",
        "\n",
        "# Step 2: Separate features and target\n",
        "# Replace 'target_column' with the actual column name for the target variable\n",
        "X = data.drop('target_column', axis=1)\n",
        "y = data['target_column']\n",
        "\n",
        "# Step 3: Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 4: Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=200)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "KcDvyMKPuhxJ",
        "outputId": "74234a3f-b562-460c-d5b2-19b863617718"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'your_file.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e16caf38f7aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Step 1: Load dataset from CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Replace 'your_file.csv' with the path to your CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'your_file.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Step 2: Separate features and target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_file.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.Write a Python program to apply Randomized Search CV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Define the model\n",
        "model = LogisticRegression(max_iter=500)\n",
        "\n",
        "# Step 4: Define hyperparameter distributions\n",
        "param_distributions = {\n",
        "    'C': loguniform(0.01, 10),  # Try values of C from 0.01 to 10 (log scale)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
        "    'solver': ['saga', 'liblinear'],  # solvers that support l1 and elasticnet\n",
        "    'l1_ratio': [None, 0.0, 0.5, 1.0]  # Only used when penalty='elasticnet'\n",
        "}\n",
        "\n",
        "# Step 5: Run Randomized Search\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    random_state=42,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Evaluate best model on test set\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(\"\\nBest Hyperparameters:\", random_search.best_params_)\n",
        "print(f\"Test Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iONxxs4Duncp",
        "outputId": "5dc1b012-3d3f-4386-b60e-e62bcac50990"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Best Hyperparameters: {'C': np.float64(0.6647135865318028), 'l1_ratio': None, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Test Accuracy: 1.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1196: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "65 fits failed out of a total of 100.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "10 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "40 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1382, in wrapper\n",
            "    estimator._validate_params()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 436, in _validate_params\n",
            "    validate_parameter_constraints(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 98, in validate_parameter_constraints\n",
            "    raise InvalidParameterError(\n",
            "sklearn.utils._param_validation.InvalidParameterError: The 'penalty' parameter of LogisticRegression must be a str among {'l1', 'elasticnet', 'l2'} or None. Got 'none' instead.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
            " 0.7        0.33333333        nan 0.95833333        nan        nan\n",
            " 0.975             nan 0.96666667        nan        nan        nan\n",
            " 0.95       0.91666667]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10.Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Step 2: Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: Create Logistic Regression model\n",
        "base_model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Step 4: Wrap with One-vs-One strategy\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "ovo_model.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Predict and evaluate\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print accuracy\n",
        "print(f\"One-vs-One Logistic Regression Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WfJNYDSHuxn_",
        "outputId": "6d29d4ca-50c0-488a-af89-a1309dd2d132"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Logistic Regression Accuracy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11.Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for Logistic Regression')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "IQsKiLOhu2ip",
        "outputId": "9c8bbe71-f6a8-4a85-a7b6-ab002d6bc9dc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12.Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Generate a synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, n_classes=2, random_state=42)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN-HzE-4u8G_",
        "outputId": "f84df50d-20ac-4c00-8e2d-48e53067d660"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.8228\n",
            "Recall:    0.9155\n",
            "F1-Score:  0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13.Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Create an imbalanced binary classification dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],  # 90% of class 0, 10% of class 1\n",
        "    flip_y=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression without class weights (baseline)\n",
        "model_no_weights = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_no_weights.fit(X_train, y_train)\n",
        "y_pred_no_weights = model_no_weights.predict(X_test)\n",
        "\n",
        "print(\"Performance without class weights:\")\n",
        "print(classification_report(y_test, y_pred_no_weights))\n",
        "\n",
        "# Initialize Logistic Regression with balanced class weights\n",
        "model_with_weights = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "model_with_weights.fit(X_train, y_train)\n",
        "y_pred_with_weights = model_with_weights.predict(X_test)\n",
        "\n",
        "print(\"Performance with class weights:\")\n",
        "print(classification_report(y_test, y_pred_with_weights))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6FsMf4NvBqc",
        "outputId": "d1fd1050-f042-47a1-e660-973a69ccf5f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance without class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.99      0.96       267\n",
            "           1       0.84      0.48      0.62        33\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.89      0.74      0.79       300\n",
            "weighted avg       0.93      0.93      0.93       300\n",
            "\n",
            "Performance with class weights:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.90      0.94       267\n",
            "           1       0.50      0.82      0.62        33\n",
            "\n",
            "    accuracy                           0.89       300\n",
            "   macro avg       0.74      0.86      0.78       300\n",
            "weighted avg       0.92      0.89      0.90       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14.Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Load Titanic dataset from seaborn (alternative: pd.read_csv if you have local file)\n",
        "import seaborn as sns\n",
        "titanic = sns.load_dataset('titanic')\n",
        "\n",
        "# Select relevant features and target\n",
        "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
        "target = 'survived'\n",
        "X = titanic[features]\n",
        "y = titanic[target]\n",
        "\n",
        "# Preprocessing steps:\n",
        "# - Fill missing values for numerical columns with median\n",
        "# - Fill missing values for categorical columns with most frequent\n",
        "# - One-hot encode categorical variables\n",
        "\n",
        "numeric_features = ['age', 'sibsp', 'parch', 'fare']\n",
        "categorical_features = ['pclass', 'sex', 'embarked']\n",
        "\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Create a pipeline with preprocessing and Logistic Regression model\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1-Score:  {f1:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S6fGgturvMwY",
        "outputId": "aa3f3f68-af33-4476-896a-ac6d09dc2eab"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.8060\n",
            "Precision: 0.7980\n",
            "Recall:    0.7117\n",
            "F1-Score:  0.7524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15.Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1) Logistic Regression WITHOUT scaling\n",
        "model_no_scaling = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "# 2) Logistic Regression WITH StandardScaler using pipeline\n",
        "model_with_scaling = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('logreg', LogisticRegression(max_iter=1000, random_state=42))\n",
        "])\n",
        "model_with_scaling.fit(X_train, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test)\n",
        "acc_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scaling:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_with_scaling:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymlXN36ovS-K",
        "outputId": "cc5c2b43-5b7e-4c1f-8b8c-7cecfd91884a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.8667\n",
            "Accuracy with scaling:    0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate synthetic binary classification dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_probs = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_probs)\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Optional: Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
        "plt.plot(fpr, tpr, label=f\"ROC curve (area = {roc_auc:.4f})\")\n",
        "plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random guessing\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve - Logistic Regression')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "uv-PX6mhvZdv",
        "outputId": "e2852d46-6233-4cf0-91f8-13dbda2119c7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9442\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Logistic Regression with C=0.5 (inverse of regularization strength)\n",
        "model = LogisticRegression(C=0.5, max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy with C=0.5: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWt4KP14vgPH",
        "outputId": "bdf25eeb-a3fe-4fcb-8692-ece70c628060"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.Write a Python program to train Logistic Regression and identify important features based on model coefficients\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic dataset\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "feature_names = [f'Feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get coefficients\n",
        "coefficients = model.coef_[0]\n",
        "\n",
        "# Pair features with coefficients and sort by absolute coefficient value descending\n",
        "feature_importance = sorted(zip(feature_names, coefficients), key=lambda x: abs(x[1]), reverse=True)\n",
        "\n",
        "print(\"Feature importance based on coefficients:\")\n",
        "for feature, coef in feature_importance:\n",
        "    print(f\"{feature}: {coef:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW7vlfALvkYZ",
        "outputId": "771d27a8-0d82-4821-c48f-73ba4305668d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importance based on coefficients:\n",
            "Feature_1: 2.3143\n",
            "Feature_8: 0.6646\n",
            "Feature_7: -0.4837\n",
            "Feature_9: 0.4111\n",
            "Feature_5: -0.2207\n",
            "Feature_2: -0.2072\n",
            "Feature_0: -0.1825\n",
            "Feature_3: -0.1606\n",
            "Feature_4: 0.0762\n",
            "Feature_6: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.Write a Python program to train Logistic Regression and evaluate its performance using Cohens Kappa Score\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Cohen's Kappa Score\n",
        "kappa = cohen_kappa_score(y_test, y_pred)\n",
        "print(f\"Cohen's Kappa Score: {kappa:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_J_vinMv1vl",
        "outputId": "12d7bba1-66a5-4f2c-94ab-f6c7f307ce66"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.7341\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classificatio:\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities for positive class\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute precision-recall curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Compute average precision score\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.plot(recall, precision, label=f'Logistic Regression (AP = {avg_precision:.4f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "tTUnSH7Wv2h2",
        "outputId": "1590a5f8-c2d3-420a-fa4c-9dc9ac3e82ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# List of solvers to try\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "\n",
        "# Dictionary to store accuracy results\n",
        "accuracy_results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    # Initialize Logistic Regression with given solver\n",
        "    model = LogisticRegression(solver=solver, max_iter=1000, random_state=42)\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_results[solver] = accuracy\n",
        "\n",
        "# Print accuracy results\n",
        "print(\"Accuracy for different solvers:\")\n",
        "for solver, acc in accuracy_results.items():\n",
        "    print(f\"{solver}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgdZ_7khv8fU",
        "outputId": "2fdc85f3-132a-4ed5-f0b7-ad45e9ab8265"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for different solvers:\n",
            "liblinear: 0.8667\n",
            "saga: 0.8667\n",
            "lbfgs: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC)\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train Logistic Regression model\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict labels on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VynerBNPwE43",
        "outputId": "b725a26f-60cd-4503-8890-17a793e83f8c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.7383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1) Logistic Regression on raw data\n",
        "model_raw = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_raw.fit(X_train, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "# 2) Logistic Regression on standardized data\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy on raw data:        {acc_raw:.4f}\")\n",
        "print(f\"Accuracy on standardized data: {acc_scaled:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p2sdVaXwKJG",
        "outputId": "3f8f0d57-3a92-4d05-a94f-66292e7f1a3b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data:        0.8667\n",
            "Accuracy on standardized data: 0.8667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Generate synthetic binary classification data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define Logistic Regression model\n",
        "logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Define parameter grid for C (regularization strength)\n",
        "param_grid = {'C': [0.01, 0.1, 0.5, 1, 5, 10, 50, 100]}\n",
        "\n",
        "# Setup GridSearchCV with 5-fold cross-validation\n",
        "grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameter C\n",
        "best_C = grid_search.best_params_['C']\n",
        "print(f\"Best C found by cross-validation: {best_C}\")\n",
        "\n",
        "# Evaluate on test set using best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test set accuracy with best C: {test_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vWYSpOjwQd1",
        "outputId": "55525876-b991-4a8d-c9d9-b52f4dad7745"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C found by cross-validation: 0.01\n",
            "Test set accuracy with best C: 0.8600\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y = make_classification(n_samples=500, n_features=10, random_state=42)\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Save the trained model to a file\n",
        "joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "print(\"Model saved to 'logistic_regression_model.joblib'\")\n",
        "\n",
        "# Load the model from the file\n",
        "loaded_model = joblib.load('logistic_regression_model.joblib')\n",
        "print(\"Model loaded from file\")\n",
        "\n",
        "# Make predictions with the loaded model\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5I2mbc8zwV1Q",
        "outputId": "b03a05d9-ce79-445d-bfb8-f48a671eddf0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to 'logistic_regression_model.joblib'\n",
            "Model loaded from file\n",
            "Accuracy of loaded model: 0.8667\n"
          ]
        }
      ]
    }
  ]
}